%% ************************************************************************
%% &&&&&&&&&&&&&&&&   01. ÁLGEBRA POLINOMIAL  &&&&&&&&&&&&&&&&&&&&&&&&&&&&&
\subsection{Construcción del Álgebra}

Como se menciona en la introducción, para poder hacer el transporte alrededor de todo una vecindad $\mathcal{U}$ de manera numérica es importante diseñar un integrador de las ecuaciones que cargue toda la información de $\mathcal{U}$ para cada paso temporal. Esto se puede lograr si en lugar de hacer evolucionar al integrador con escalares en $\mathbb{R}$ o $\mathbb{C}$ se hace con un polinomio con centro en la condición inicial $\mathbf{x_0} := \mathbf{x}(t=0)$ , de modo que para cada paso se tiene un nuevo polinomio que representa la evolución del polinomio en el paso anterior, el cual se puede sustituir por reales y obtener así puntos de la vecindad de $\mathbf{x_0}$. \\

Dicho lo anterior, es necesario desarrollar un álgebra polinomial que denotaremos como $\algpol$ donde $\pk$ es el conjunto de \textbf{polinomios de orden $n$ con coeficientes en el campo $\mathbb{K}$} tal que, si  $P(x) \in \pk$, éste se define como
$$P = P(x) := \polk{p_k} $$
con $P: \mathbb{C} \to \mathbb{K}$ una función analítica y $p_k \in \mathbb{K} \  \forall i \in [0,n]$.

%		- It may be necessary to analize how different independet variables imply that A(x) != A(b).


Inspirado en la aritmética usual en $\mathbb{C}$ o $\mathbb{R}$, podemos definir $(+,\cdot)$ para $\algpol$ de la siguiente manera:

Sean $A,B \in \pk$ con $A = \polk{a_k}$ y $B = \polk{b_k} \ a_k,b_k \in \mathbb{K} \ \forall k \in [0,n]$, entonces, para la suma, notemos que
\begin{align*}
\polk{a_k} + \polk{b_k} =& a_0 + a_1 x + \cdots a_n x^n + b_0 + b_1 x + \cdots + b_n x^n \\
=& (a_0 + b_0) + (a_1 + b_1) x + \cdots + (a_n + b_n)x^n = \polk{(a_n + b_n)} 
\end{align*}
así, se define \textbf{$+$}  en $\pk$ como
\begin{equation}
A + B = \polk{(a_k + b_k)}.
\label{eq: polsum}
\end{equation}

Para el producto, notemos que
\begin{align*}
\polk{a_k}\cdot\polk{b_k} =& (a_0 + a_1 x + \cdots + a_n x^n)\cdot(b_0 + b_1 x + \cdots + b_n x^n) \\
=& (a_0b_0) + (a_1b_0 + a_0b_1) x + (a_2b_0 + a_1b_1 + a_0b_2) x^2 + \cdots + \\
+& \sum_{j=0}^n a_{n-j}b_j x^n + \mathcal{O}(x^{n+1}) = \polk{ \polj{a_{k-j}b_j } }
\end{align*}

así, se define \textbf{$\cdot$}  en $\pk$ como

\begin{equation}
A \cdot B = \polk{\sum_{j=0}^k a_{k-j}b_j}.
\label{eq: polprod}
\end{equation}
Aún cuando las operaciones están definidas inspiradas en la aritmética de los números, $\mathbb{K}$ puede ser cualquier campo arbitrario.

\begin{proposicion}
Para $\mathbb{K} = \mathbb{R}$ o $\mathbb{C}$, $\algpol$ forma un campo.
\end{proposicion}

\begin{proof}
Basta probar las nueve propiedades de campo con las operaciones de \ref{eq: polsum} y \ref{eq: polprod}.
Sean $A$, $B$ y $C \in \pk$
\begin{enumerate}

 \item $ A + B = B + A $
 \begin{proof}
  \begin{align*}
   A + B =& \polk{a_k} + \polk{b_k}  = \polk{(a_k + b_k)} \\ 
   =& \polk{(b_k + a_k)} = \polk{b_k} +     \polk{a_k} = B + A
  \end{align*}
 \end{proof}
 
 \item A + (B+C) = (A+B) + C
 \begin{proof}
  \begin{align*}
   A + (B+C) =& \polk{a} + \polk{(b_k+c_k)} = \polk{(a_k + (b_k + c_k))} \\
   =&  \polk{((a_k + b_k) + c_k)} = \polk{(a_k+b_k)} + \polk{c_k} \\
   =& (A+B) + C 
  \end{align*}
 \end{proof}
 
 \item $\exists \  \mathbf{0} \in \pk \text{ tal que }  \mathbf{0} + A = A $
 \begin{proof}
 Sea $\mathbb{0} = \mathbf{0}(x) = \polk{\sigma_k}$ donde $\sigma_k = 0 \ \forall k \in [0,n]$, así 
  \begin{align*}
  \mathbf{0} + A = \polk{(\sigma_k + a_k)} = \polk{(0 + a_k)} = \polk{a_k} = A
  \end{align*}
 \end{proof}
 
 \item $\exists \ {-A} \in\pk  \text{ tal que } A + (-A) = 0 $
 \begin{proof}
 Sea $-A = -A(x) = \polk{\mathrm{a}_k}$ donde $\mathrm{a}_k = -a_k \ \forall k \in [0,n]$, así
  \begin{align*}
   A + (-A) = \polk{(a_k + \mathrm{a}_k)} = \polk{(a_k + (-a_k))} = \polk{0} = \mathbf{0}
  \end{align*}
 \end{proof}
 
 \item $ A\cdot B = B\cdot A $
 \begin{proof}
 Hay que probar, básicamente, que $\sum_{j=0}^k{a_{k-j}b_j} = \sum_{j=0}^k{b_{k-j}a_j}$:
  \begin{align*} 
   \sum_{j=0}^k{a_{k-j}b_j} =& \sum_{i=0}^k{a_ib_{k-i}} \text{ con } i = k - j \\
   =& \sum_{i=0}^k{ b_{k-i}a_i } = \sum_{j=0}^k { b_{k-j}a_j }
  \end{align*}
  $\therefore A\cdot B = B\cdot A$.
 \end{proof}
 
 \item $ (A \cdot B) \cdot C = A \cdot (B \cdot C) $
 \begin{proof}
 Por un lado
  \begin{align*}
  A \cdot (B \cdot C) =& \polk{a_k}\ \polk{ \polj{b_{k-j}c_j} } = \polk{\big(  \sum_{j=0}^ka_{k-j}\sum_{i=0}^jb_{j-i}c_i \big)} \\
  =& \polk{\big(  \sum_{r+j=k}a_r\sum_{p+i=j}b_pc_i \big)} = \polk{\big(  \sum_{r+j=k}\sum_{p+i=j}a_rb_pc_i \big)} \\
  =& \polk{\big(  \sum_{r+p+i=k}a_rb_pc_i \big)} 
  \end{align*}
  por otro
    \begin{align*}
  (A \cdot B) \cdot C =& \polk{ \polj{a_{k-j}b_j} }\ \polk{c_k} = \polk{\big( \sum_{i=0}^ja_{j-i}b_i  \sum_{j=0}^kc_{k-j} \big)} \\
  =& \polk{\big(  \sum_{r+j=k}(\sum_{p+i=r}a_pb_i) c_j \big)} = \polk{\big(  \sum_{r+j=k}\sum_{p+i=r}a_pb_ic_j \big)} \\
  =& \polk{\big(  \sum_{p+i+j=k}a_pb_ic_j \big)} 
  \end{align*}
  
  Basta ver que, por un cambio de nombre de índices $p \to r$, $i \to p$, $j \to i$,
  \begin{align*}
   \sum_{p+i+j=k}a_pb_ic_j = \sum_{r+p+i=k}a_rb_pc_i
  \end{align*}   
  $\therefore (A \cdot B) \cdot C = A \cdot (B \cdot C)$.
 \end{proof}
 
 \item $\exists \ \textbf{1} \in \pk \text{ tal que } \textbf{1}\cdot A = A $
 \begin{proof}
 Sea $ \textbf{1} = \textbf{1}(x) = \polk{\omega_k}$ donde $\omega_k = 0 \ \forall k > 0$ y $\omega_0 = 1$, así:
 \begin{align*}
  \textbf{1} \cdot A = \polk{ \polj{a_{k-j}\omega_j} } = \polk{ a_{k-0} \cdot 1 } 
  =& \polk{a_k} = A 
 \end{align*}
 \end{proof}
 
 \item $\exists \  A^{-1} \in \pk \text{ tal que } A \cdot A^{-1} = \textbf{1} \ \forall A \text{ con } a_o \neq 0 $
 \begin{proof}
 Sea $A^{-1} = A^{-1}(x) = \polk{\alpha_k}$; con 
 \end{proof}
 \begin{align*}
  A \cdot A^{-1} = \polk{ \polj{a_{k-j}\alpha_j} } \text{ así, buscando} \\
  \sum_{j=0}^k a_{k-j} \alpha_j = 0 \ \forall \ k>0 \text{ y }  \alpha_0 = \frac{1}{a_0}  
 \end{align*}
 podemos llegar a una fórmula recursiva para $\alpha_k$
 \begin{align*}
 \alpha_k = -\frac{1}{a_0} \sum_{j=0}^{k-1}a_{k-j}\alpha_j.
 \end{align*}
 Así, por construcción, se obtiene que $ A \cdot A^{-1} = \textbf{1} $.
 \item $ A \cdot (B+C) = A \cdot B + A \cdot C $
 \begin{proof}
  \begin{align*}
  A \cdot (B+C) =& \polk{a_k} \big(\polk{(b_k + c_k)} \big) = \polk{\polj{a_{k-j}(b_j+c_j)}} \\
  =& \polk{ \big( \polj{a_{k-j}b_j} \polj{a_{k-j}c_j} \big) } = A \cdot B + A \cdot C
  \end{align*}   
 \end{proof}
 
\end{enumerate}
Así, quedan demostradas todas las propiedades de campo. \qed
\end{proof}

Visto eso, es práctico definir otras operaciones que serán de ayuda al trabajar con elementos de $\pk$. Serán la potencia, la composición y la derivada nuestros ` caballos de batalla '' para el desarrollo de casi cualquier función existente en la aritmética usual.

Sea $A = A(x) = \polk{a_k} \in \pk$. Si $m \in \mathbb{N}$,
\begin{align*}
 A^m =& \big(\polk{a_k}\big)^m = \polk{a_k}\polk{a_k}\big(\polk{a_k}\big)^{m-2} \\
 =& \polk{\polj{a_{k-j}a_j}}\polk{a_k}\big(\polk{a_k}\big)^{m-3} = \polk{{^1\alpha_k}}\polk{a_k}\big(\polk{a_k}\big)^{l-3} \\
 =& \cdots = \polk{{^m\alpha_k}} \\
 &\text{con } {^1\alpha_k} = \polj{a_{k-j}a_j} \text{ y } {^m\alpha_k} = \polj{{^{m-1}\alpha_k}a_j} \\
\end{align*}
y , aunque no queda clara la forma explícita de ${^m\alpha_k}$, se muestra que la \textbf{potencia} está bien definida y que $A^m \in \pk$.


Con esto último, y sea $B = \polk{b_k} \in \pk$, se puede desarrollar
\begin{align*}
 B(A(x)) = B(A) =& \sum_{k=0}^n b_k \big( \sum_{j=0}^n a_j x^j  \big)^k = \polk{b_k} \sum_{j=0}^n {^k\alpha_j x^j} =\polk{ \polj{b_{k-j} {^k\alpha_j} } }
 \end{align*}
que corresponde a la \textbf{composición} de $A$ en $B$, la cual está bien definida y pertenece a $\pk$.

Inspirados en la derivada usual de polinomios de orden $n$ $P: \mathbb{C} \to \mathbb{C}$, tenemos que

\begin{align*}
 \frac{dP(x)}{dx} := P'(x) = P' = \sum_{k=0}^n{k p_k x^{k-1}} = \polk{(k+1)p_{k+1}} 
\end{align*}

así, se define la \textbf{derivada} de $A$ como

\begin{align}
 A' = \polk{(k+1)a_{k+1}}
 \label{eq:polderiv}
\end{align}

con $a_{n+1} := 0 \in \mathbb{K}$ tal que $A' \in \pk$. De la definición usual de derivada podemos ver cómo la \textbf{regla de la cadena} también tiene sentido en este campo, ya que 
\begin{align*}
 { A(B(x))}' :=& \lim_{x \to a}\frac{A(B(x)) - A(B(a))}{x-a} = \lim_{x \to a} \frac{A(B(x)) - A(B(a))}{B(x)-B(a)}\frac{B(x) - B(a)}{x-a}  \\
 =& \lim_{x \to a} \frac{A(B(x)) - A(B(a))}{B(x)-B(a)} \lim_{x \to a} \frac{B(x) - B(a)}{x-a} = A(B(x))'B(x)'
\end{align*}

lo cual se puede traducir a 
\begin{align*}
\lim_{x \to a} \big( A(B(x)) + \ - A(B(a)) \big) \cdot \big( (B(x) + \ -B(a) \big)^{-1} \cdot \lim_{x \to a} \big( (B(x) + \ -B(a) \big) \cdot \big( \textbf{1(x)} + \ -\textbf{1(a)} \big)    
\end{align*}
con las operaciones definidas hasta ahora.

%		- See "chain rule" technical details. Maybe only mention taht is has sense for polynomial functions and later on prove that it is indeed true.
%		- Define (/,-,sin,cos,exp,log,pow). This should be expanded when needed in the thesis only.

Aún cuando ya tenemos las operaciones básicas del álgebra definidas, va a ser de gran practicidad extender otras operaciones útiles para el transporte de $\mathbb{U}$ para el campo vectorial que estemos estudiando.

Se define la \textbf{resta $-$} como
\begin{equation}
 A - B := A + (-B).
 \label{eq:polisub}
\end{equation}


Para la división, sea $D(x) \in \pk$, tal que  $B \cdot D = A $,  entonces 
\begin{align*}
 & \polk{ \polj{b_{k-j} d_j} } = \polk{ a_k } \implies \polk{ a_k - \polj{b_{k-j} d_j} } = 0 \\ 
 &\implies a_k - ( \sum_{j=0}^{k-1}a_{k-j} d_j + b_0 d_k ) = 0 \\
 & \therefore d_k = \frac{1}{b_0} \big( a_k - \sum_{j=0}^{k-1} b_{k-j} d_j \big)  
\end{align*}

Se define la \textbf{división} como
\begin{equation}
 D = A/B \text{ con } d_k = \frac{1}{b_0} \big( a_k - \sum_{j=0}^{k-1} b_{k-j} d_j \big)
 \label{eq:polidiv}
\end{equation}

Con el mismo espíritu, inspirados por el artículo de Haro ~\cite{Haro2009} podemos definir más relaciones de recurrencia para otras operaciones entre polinomios.

%		- Discussion about the "x" in the polynomials and how they represent different things and can be altered. 
%		- Here, polynomials will be the normal functions with its normal derivatives. 
%		- Maybe I can make just a few and then cite "Haro"... in the spirit of Haro, other functions will be defined as ---


Fantástico, $^nP_{\mathbb{C}}$ y $^nP_{\mathbb{R}}$ son campos y tenemos ya varias operaciones definidas sobre $\pk$, pero la motivación del desarrollo de este álgebra es hacer polinomios de una variable (que representen al tiempo) cuyos coeficientes sean polinomios de varias variables (que representen al espacio fase de $\dot{x} = f(x(t))$), cuyos coeficientes sean elementos de $\mathbb{C}$ o $\mathbb{R}$. Es decir, nos gustaría trabajar con polinomios cuyos coeficientes sean polinomios, cuyos coeficientes sean, finalmente, números.
Sea  $\pk$ con $\mathbb{K} = {{^{n}P_{\mathbb{C}}}}$, entonces

\begin{align*}
 P(x) = \polk{p_k(y)} = \polk{\sum_{j=0}^n p_{kj} y^j}
\end{align*}

así, se observa cómo un polinomio cuyos coeficientes son polinomios es equivalente a un polinomio de dos variables que vive en un espacio que denominaremos, por construcción, $\pkn{2}$.

Esto último se puede extender inductivamente para construir polinomios de cualquier orden. Para el estudio de esta tesis buscamos construir $\pk$ donde $\mathbb{K} = \pkn{N}$ con $N$ el grado de la ecuación diferencial a desarrollar.


%		- Prove that this algebra is also a field over \pk (mention) - done -
%		- Show that a polynomial of a polynomial of 1 variable is a polynomial of 2 variables... or n variables (inductive way)!  - done -
%		  (we're gonna use Taylor1{TaylorN{K}})
%		- Prove, as a corolary, that this algebra is also a vector space over K (optional). - not necessary -


%% ****** NEXT UP ********
%		- Investigate about error of a \pk for 1 variable and then for n variables.

\subsection{Integrador de Taylor}

%		- Cómo funciona un integrador numérico genérico (usar Euler como ejemplo, quizá)?
%		- ¿Qué es el step size y cómo se determina?
%		- ¿Cómo calcular el error en cada paso?
%		- ¿En qué unidades trabajar?
%		- ¿Cómo funciona ÉSTE integrador de superficies iniciales? 

Ahora que ya están definidas todas las operaciones de polinomios es buen momento para desarrollar al integrador numérico que nos permitirá hacer el transporte mencionadas en la introducción.

La idea básica de un integrador es que, dado un sistema de ecuaciones diferenciales

\begin{align}
 \dot{\textbf{x}} := \frac{dx_i(t)}{dt} = f_i(\textbf{x}(t)) \ \forall i
 \label{eq: eqdif}
\end{align}

se encuentre un método que evolucione de la mejor manera posible dada una condición inicial $x_0$ o, en nuestro caso, una \textit{bola} inicial $B_{x_0}$ a cada paso del parámetro $t$ (tiempo) que rige la ecuación. El tema con un integrador numérico es que éste pretende evaluar todos los tiempos contenidos entre la condición inicial y la final y, como ninguna máquina ha sido capaz de evaluar una cantidad continua en un intervalo, es necesario discretizar al parámetro que hace que nuestras ecuaciones evolucionen.

Una forma muy intuitiva de abordar este problema es con el método de Euler, que parte de la definición usual de derivada. Tomemos el caso de una función analítica $f: \mathbb{R}  \to \mathbb{R}$, entonces

\begin{align}
 \dot{f} := \frac{df(t)}{dt} = \lim_{h\to 0} \frac{ f(t+h) - f(t) }{h}
 \label{eq: deriv}
\end{align}.

Si tomamos el límite $h \to 0$ de una manera menos rigurosa y lo tratamos simplemente como un \textit{incremento muy pequeño}, podemos tener una aproximación razonablemente buena de la derivada, donde

\begin{align*}
\dot{f} = \frac{ f(t+h) - f(t) }{h} + R(t;h)
\end{align*}

donde $R$ es un residuo que depende de $h$ y decrece con ella. Si esto lo aplicamos a \ref{eq: eqdif} cuando $\textbf{x} \in \mathbf{R}$, entonces

\begin{align}
 f(x(t)) =& \frac{dx(t)}{dt} = \frac{ x(t+h) - x(t) }{h}+ R(h)\\
 \implies& x(t+h) = x(t) + h\cdot f(\textbf{x}(t)) - R(t;h)
 \label{eq: poli1g}
\end{align}
\begin{align}
 \to \ & x_{n+1} = x_n + h\cdot f_n - R_1(h)
 \label{eq: euler}
\end{align}

que plantea una relación de recurrencia, conocida como el \textbf{método de Euler}. Aún cuando esta relación no es tan poderosa y no es, definitivamente, la que se usará en los cálculos desarrollados en la presente tesis, es interesante notar que \ref{eq: euler} es ahora un \textit{mapeo} que resuelve la ecuación diferencial de manera discreta, con un \textit{salto temporal} de tamaño $h$ y un error o residuo $R_1(h)$ y esto se conservará para cualquier método empleado.\\

%	- Releer con cuidado esta sección para ver si la redacción puede mejorar. Introducir el método de Taylor en 1 variable, el método de taylor en varias variables y el método de Taylor con coeficientes polinomiales debe ser algo natural. Posiblemente hablar del error tenga sentido después de presentar el de varias variables y discutir cómo cambiaría este cuando los coeficientes son polinomios (ver 244-246 de D. Perez en esto). Entender bien qué onda con el radio de convergencia planteado en este paper en relación con lo que dice Simó. 

Hay que prestar especial atención al error que pueda tener la ecuación en cada paso temporal, ya que éste determinará qué tan preciso es el método y qué tan fiable es la solución que obtenemos al integrar para un intervalo de tiempo dado. Si se observa la ecuación \ref{eq: poli1g}, se puede reconocer el desarrollo en \textit{series de Taylor} a primer orden de $x(t)$. Éste se puede generalizar a cualquier orden reduciendo el error al aumentar el orden del desarrollo de la serie de Taylor de la función del sistema dinámico. Es de especial interés entender cómo varía el error en función del orden del polinomio escogido (notar que en \ref{eq: euler} se escribe $R_1(h)$ que corresponde al error dado por un desarrollo de primer orden) y del tamaño del paso de integración escogido $h$.

En el apéndice \textit{Taylor series methods of integration} de Simó (2001) ~\cite{Simo2001} se propone y prueba que el paso de integración más eficiente es 
\begin{equation}
 h = \frac{\rho}{e^{2}}
\end{equation}

con $\rho = \rho(t)$ el radio de convergencia de la serie asociada a la función desarrollada. 